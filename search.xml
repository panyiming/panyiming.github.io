<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Age Estimation]]></title>
    <url>%2F2019%2F08%2F28%2Fage-estimation%2F</url>
    <content type="text"><![CDATA[1 open data link IMDB-WIKI CHALEARN MORPH 2 Introduction of age estimationThere are three difficuties of age estimation: First, compared with image classification or face recognition, existing age estimation datasets are always limited because it is very hard to gather complete and sufficient age labeled dataset. Second, the number of images is very imbalanced in differnt age groups, which will bring a serious challenge for developing an unbiased estimation system. Number of age distribution of IMDB-WIKI is as follows: Third, age estimation is a very fine-gained recongniton task. It is even very difficult for human beings to recognize the exact age number of a face. So to develop a roubust and accurate age estimation system, we must consider the age estimation is different from common image classification problems. For example, the differnce between the face of 26 and 27 is too small to recognize. To solve this problem, there are mainly two kinds of method: ranking based age estimation label distribution learning Besides, the effect of commbination between these two method is very effective too. 3 ranking based age estimation3.1 Ranking-CNN IntriductionRanking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs. Ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approache. Ranking-CNN architechture Basic Binary CNNsA basic CNN has three convolutionaland sub-sampling layers, and three fully connected layers. Specifically, C1 is the first convolutional layer with feature maps connected to a 5×5 neighboring area in the input. There are 96 filters applied to each of the 3 channels (RGB) of the input, followed by ReLU. S2 is a sub-sampling layer with feature maps connected to corresponding feature maps in C1. In this paper, the authors use max pooling on 3×3 regions with the stride of 2 to emphasize the most responsive points in the feature maps. S2 is followed by local response normalization that can aid generalization. C3 works in a similar way as C1 with 256 filters in 96 channels and 5×5 filter size followed by ReLU. Layer S4 functions similarly as S2, and is followed by LRN. Then, C5 is the third convolutional layer with 384 filters in 256 channels and smaller filter size 3×3, followed by the third max pooling layer S6. F7 is the first fully connected layer in which the featuremaps are flattened into a feature vector. There are 512 neurons in F7 followed by ReLU and a dropout layer. F8 is the second fully connected layer with 512 neurons that receives the output from F7 followed by ReLU and another dropout layer. F9 is the third fully connected layer and computes the probability that an input x belongs to class i using the logistic function. The optimal model parameters of a network are typically learned through minimizing a loss function. The authours use the negative log-likelihood as the loss function, and minimize it using stochastic gradient descent. whole network structure If the age label number is K, then there will be K-1 basic CNN networks to be trained. To train the k-th binary CNN, the entire dataset D is split into two subsets, with age values higher or lower (or equal to) than k. Each basic network is trained using the entire dataset. D^{+}_{k} = \{ x_\{i\}+1|y_{i}>k\} , D^{-}_{k}= \{ x_{i}+1|y_{i} \leq k \} inferenceGiven an unknown input xi, we first use the basic networks to make a set of binary decisions and then aggre-gate them to make the final age prediction. f(x) is the output of basic network and [.] denotes the true-test operator, which is 1 if the inner condition is true, and 0 otherwise. :r(x_{i}) = 1 + \sum^{K-1}_{k=1}[f_{k}(x_{i})>0] 4 Label Distribution Learning age estimation 4.1 Mean-Variance Loss introduction In our approach, the authors also use the weights of softmax to calculate a weighted average age, but they use the method in both the training and the testing phases. Besides, they penalize the variance so that the estimated age distribution could take a sharp shape, which is important to obtain an age estimate as accurate as possible. In addition, the proposed approach does not require each training face image to have a mean age and a variance value. Distribution learning is proposed to address problems due to label ambiguity. Different from single label learning or multi-label learning, which assigns a single label or multiple labels to an object, distribution learning assigns a label distribution to an object. Compared to single label learning and multi-label learning, distribution learning is able to leverage the relative relationship of a sequence of values in the label space, leading to more robust estimation. The framwork proposed in this paper is as follows: The paper proposed mean-variance loss, together with the softmax loss, is embedded into a CNN for end-to-end learning. Mean-Variance Loss Formally, let x_{i}denote the feature vector of the i-th sample, y_{i}\in {1,2,...,k} denote the corresponding age label, and f(x_{i})\in R^{N\times M} denote the output of a CNN ahead of the last fully connected FC layer. The output of the last FC layer (z\in R^{N\times K}), and a typical softmax (P\in R^{N\times K}) probability can be computed using: $$z=f(x_{i})\theta ^{T}, p_{i,j} = {{e^{z_{i,j}}}\over{\sum^{K}_{k=1} e_{z_{i,j}}}}$$ where\theta \in R^{K \times M} is the parameter of the last FC layer, and z_{i,j} is one element of z; j \in {1,2, ...,K} denotes the class labels (here it denotes the age); So pi denotes the estimated age distribution for sample i over all the K classes, and p_{i,j} denotes the probability that sample i belongs to class j. We can compute the mean (m_i) and the variance(v_j) of a distribution (p_i) as follow m_{i} = \sum^{K}_{j=1}j*p_{i,j}v_{i} = \sum^{K}_{j=1}p_{i,j}*(j-m)^2 Mean Loss:The mean loss component in mean-variance loss penalizes the difference between the mean of an estimated age distribution and the ground-truth age. The mean loss can be computed as:L_m = {1 \over 2N} \sum^{N}_{i=1}(m_i-y_i)^2 = {1 \over 2N} \sum^{N}_{i=1}(\sum^{K}_{j=1}j*p_{i,j}-y_i)^2 Variance Loss:The variance loss component in our mean-variance loss penalizes the dispersion of an estimated age distribution.the variance loss can be computed asL_v = {1 \over N} \sum^{N}_{i=1} v_i = {1 \over N} \sum^{N}_{i=1} \sum^{K}_{j=1}p_{i,j}*(j-\sum^{K}_{k=1}k*p_{i,k}^2) Specifically, we embed our mean-variance loss into the architecture of CNNs, and use the softmax loss and mean-variance loss jointly as the supervision signal. L = L_s+\lambda_1L_m+\lambda_2L_vwhere \lambda_1 and \lambda_2 are two hyper-parameters, balancing the influencing of individual sub-losses in the joint loss. 4.2 Deep Label Distribution Learning IntriductionGiven an input image, we are interested in estimating a category output y. For two input images X1 and X2 with ground-truth labels y1 and y2, X1 and X2 are supposed to be similar to each other if the correlation of y1 and y2 is strong, and vice versa. For example, the correlation between faces aged 32 and 33 should be 9 than that between faces aged 32 and 64, in terms of facial details that reflect the age. In other words, we expect high correlation among input images with similar outputs. Deep label distribution learningGiven an instance X with label distribution y, we assume that x = \Phi (X; θ) is the activation of the last fully connected layer in a deep ConvNet. We use a softmax function to turn these activations into a probability distribution, that is: \hat{y} ={ exp(x_j) \over \sum_t exp(x_t)}Given a training data set D, the goal of DLDL is to find θ to generate a distribution \hat{y} that is similar to y. if the Kullback-Leibler (KL) divergence is used as the measurement of the similarity between the ground-truth and predicted label distribution, we can define the loss function as: T = - \sum_k y_k In \hat{y}_k Label distribution constructionA desirable label distribution y = (y_1, y_2, . . . , y_C) must satisfy some basic principles: (1) y should be a probability distribution. Thus, we have yi \in [0, 1] \ \& \sum^{C}_{i=1} y_i=1.(2) The probability values y_i should have difference among all possible labels associated with an image.For age estimation, we assume that the probabilities should concentrate around the ground-truth age y.Thus, we quantize y to get y using a normal distribution.Since we use equal step size in quantizing y, the normal probability density function is a natural choice to generate the ground-truth y from \mu and \sigma: $$y_i= {{p(l_j| \mu,\sigma)}\over{\sum_k p(l_k|\mu,\sigma)}}$$ $$p(l_j|\mu, \sigma) = {1\over \sqrt{2\pi} \sigma} exp(-{{(l_j-\mu)^2}\over{2\sigma^2}})$$ the predict of several examples test results 4.3 Expectation of Label Distribution LearningNote that the Deep Label Distribution Learning only learns a label distribution but cannot regress a precise value. In order to reduce the inconsis- tency between training and evaluation stages, we propose an expectation regression module to further refine the predicted value. The Label Distribution Learning ModuleThis module follows the DLDL method. The Expectation Regression ModuleNote that the LDL module only learns a label distribution but cannot regress a precise value. In order to reduce the inconsis- tency between training and evaluation stages, we propose an expectation regression module to further refine the predicted value.The expectation layer takes the predicted distribution and label set as input and outputs its expectation:\hat{y} = \sum_k \hat{p_k} l_kGiven an input image, the expectation regression module minimizes the error between the expected value \hat{y} and ground-truthy. We use L_1 loss as the error mea- surement as follows:l_{er}=|\hat{y}-y| 6 evaluation Mean Absolute Error (MAE) MAE = {1\over N } \sum^{N}_{n=1}|\hat{l_n}-l_n|where \hat{l_n} and l_n are the estimated and ground-truth age of the n-th testing image, respectively. Cumulative Score (CS)CS is defined as the accuracy rate of correct estimation:CS_g = {C_g\over{N}}\times100 \%where C_g is the number of correct estimation, testing images that satisfy |\hat{l_n}-l_n|\leq g. $\epsilon- error$ $$\epsilon = {1\over N} \sum_{n=1}^{N} (1-exp(-{{(\hat{l_n}-l_n)^2}\over{2\sigma^2_n}}))$$ confusion matrix References1, Using Ranking-CNN for Age Estimation Shixing2, Mean-Variance Loss for Deep Age Estimation from a Face3, Age Estimation Using Expectation of Label Distribution Learning4, Deep Label Distribution Learning With Label Ambiguity]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>face_recognition</tag>
        <tag>age estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cnn Network On Mobile Device]]></title>
    <url>%2F2019%2F06%2F05%2FCnn-Network-on-Mobile-Device%2F</url>
    <content type="text"><![CDATA[1 Depthwise Separable Convolutions1.1 Normal Convolution:Every chanel of the input will be connected to the per channel of the output. If the input size is h_{1} \times w_{1} \times c_{1}, the filters size is h \times w \times c1, the output size is h_{2} \times w_{2} \times c_{2}, the parameter count will be: h \times w \times c_{1} \times c_{2}The example is as follows:input size is 4\times 4 \times 4 and the filters size is 2\times 2\times 4 \times 4, (padding=0, stride=1), output size is 3\times 3 \times 4, so the parameter count is: 2 \times 2 \times4 \times 4=641.2 Group ConvolutionIf the input size is h_{1} \times w_{1} \times c_{1}, the filters size is h\times w \times c_{1}, the output size is h_{2}\times w_{2}\times c_{2}. The input channels and filters will be divided into g groups where each the filters in each filter group are convolved with only \frac{c_{1}}{g} previous layer’s featuremaps, the parameter count will be: h \times w \times \frac{c_{1}}{g} \times \frac{c_{2}}{g} \times gThe example is as follows:input size is 4 \times 4 \times 4 and the input channels are divided into 2 groups, so the size of per group is 4 \times 4 \times \frac{4}{2}, the filters size is 2 \times 2 \times \frac{4}{2}, (padding=0, stride=1), output size is 3 \times 3\times 4, so the parameter count is: 2 \times 2 \times \frac{4}{2} \times \frac{4}{2} \times 2=32Compared with normal convolution with same input/output size, the parameter count of group convolution will reduce to \frac{1}{g}. 1.3 Seperable ConvolutionDepthwise separable convolutions factorize a standard convolution into a depthwise convolution and a 1×1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1×1 convolution to combine the outputs the depthwise convolution. For instance, a depthwise separable convolution layer takes as input a h_{1} \times w_{1} \times c_{1} feature map and produce a h_{2} \times w_{2} \times c_{2} feature map, the filters size is h \times w \times c_{1}, depthwise convolution’s parameter count is : h \times w \times 1 \times c_{1}and the pointwise convolution’s parameter count is : 1 \times 1 \times c_{1} \times c_{2}So the general parameter count of seperable convolution is h \times w \times 1\times c_{1}+1\times 1 \times c_{1} \times c_{2} , which reduces to \frac{1}{c_{2}}+\frac{1}{h \times w} of standard convolution.The example is as follows: input size is 4 \times 4 \times 4 and depthwise convolution filter size is 2 \times 2 \times 4, output size is 3 \times 3 \times 4 (padding=0, stride=1), so the parameter count is: 2 \times 2 \times 1 \times 4+1 \times 1 \times 4 \times 4=32 2 IGCV Series3 Shufflenet Series4 Mobilenet Series4.1 Mobilenet v1In the mobilenetv1, the core layers that MobileNet is built on is depthwise separable convolution, which is descriped in section 1.3. 4.1.1 Network Structure: The first layer of mobilenetv1 is a full convolution. All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Downsampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers. The MobileNet architecture is defined as follows: MobileNetv1 spends 95% of it’s computation time in 1 × 1 convolutions which also has 75% of the parameters. Nearly all of the additional parameters are in the fully connected layer. 4.1.2 Width MultiplierIn order to construct these smaller and less computationally expensive models mobilenetv1 introduce a very simple parameter \alpha called width multiplier. For a given layer, and width multiplier \alpha, the number of input channels M becomes \alpha M and the number of output channels N becomes \alpha N.Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly \alpha^2. 4.1.3 Resolution MultiplierThe input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set \rho by setting the input resolution. Resolution multiplier has the effect of reducing computational cost by \rho^2. 4.1.4 Experiments Results 4.2 Mobilenet v24.2.1 Depthwise Separable Convolutions DepthwiseMobileNetV2 uses k = 3 (3×3 depthwise separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy. 4.2.2 Linear BottlenecksThe manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space: If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation. ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is lowdimensional we can capture this by inserting linear bottleneck layers into the convolutional blocks. 4.2.3 Inverted ResidualsThe motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers. However, the inverted design is considerably more memory efficient, as well as works slightly better in author’s experiments. The basic implementation structure is as follows, bottleneck residual block transforming from k to k’ channels, with stride s, and expansion factor t,.When the expansion ratio is smaller than 1, this is a classical residual convolutional block. However, the author show that expansion ratio greater than 1 is the most useful. parameters example images 4.2.4 Model Architecture the basic building block is a bottleneck depth-separable convolution with residuals. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation. We always use kernel size 3×3 as is standard for modern networks, and utilize dropout and batch normalization during training. With the exception of the first layer, we use constant expansion rate throughout the network. For all our main experiments we use expansion factor of 6 applied to the size of the input tensor. Each line describes a sequence of 1 or more identical layers, repeated n times. All layers in the same sequence have the same number c of output channels. The first layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 × 3 kernels. The expansion factor t is always applied to the input size as described in Table. 4.4.5 Experiment The impact of non-linearities and various types of shortcut (residual) connections. ImageNet Classification 4.3 Mobilenet v34.3.1 Efficient Mobile Building Blocks Mobilenet V2 bottleneck Mobilenet V3 bottleneck Compared with mobilenetv2, there are some changes: add squeeze and excitation add hard sigmoid 4.3.2 Redesigning Feature LayersMobileNetV2’s inverted bottleneck structure and variants use 1x1 convolution as a final layer in order to expand to a higher-dimensional feature space. This layer is critically important in order to have rich features for prediction. However, this comes at a cost of extra latency. To reduce latency and preserve the high dimensional features, we move this layer past the final average pooling. This final set of features is now computed at 1x1 spatial resolution instead of 7x7 spatial resolution. The efficient last stage reduces the latency by 10 milliseconds which is 15% of the running time and reduces the number of operations by 30 millions MAdds with almost no loss of accuracy. 4.3.3 Hard SigmoidA non-linearity called swish was introduced that when used as a drop-in replacement for ReLU, that significantly improves the accuracy of neural networks. The non-linearity is defined as: swish(x) = x · σ(x)We replace sigmoid function with its piece-wise linear hard analog: hswish(x) = x \frac{Relu6(x+3)}{6} First, optimized implementations of ReLU6 are available on virtually all software and hardware frameworks. Second, in quantized mode, it eliminates potential numerical precision loss caused by different implementations of the approximate sigmoid. Finally, even optimized implementations of quantized sigmoid tend to be far slower than their ReLU counterparts. In our experiments, replacing h-swish with swish in quantized mode increased inference latency by 15%. The cost of applying nonlinearity decreases as we go deeper into the network. We find that most of the benefits swish are realized by using them only in the deeper layers. Thus in our architectures we only use h-swish at the second half of the model. 4.3.4 MobileNetV3 Architecture mobilenetv3 large mobilenetv3 small 4.3.4 Experiments Floating point performance on Pixel family of phones Quantized performance]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>mobile_net</tag>
        <tag>face_recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fate books]]></title>
    <url>%2F2019%2F06%2F01%2Ffate-books%2F</url>
    <content type="text"><![CDATA[三命汇通论五行生成 天高寥廓，六气回旋以成四时；地厚幽深，五行化生以成万物。可谓无穷而莫测者也。圣人立法以推步者，盖不能逃其数。观其立数之因，亦皆出乎自然。故载于经典，同而不异，推以 达其机，穷以通其变，皆不离于数内。一曰水，二曰火，三曰木，四曰金，五曰土者，咸有所自也。水，北方子之位也，子者，阳之初一，阳数也，故水曰一；火，南方午之位也，午者，阴之初二，阴数也，故火曰二；木居东方，东，阳也，三者，奇之数，亦阳也，故木曰三；金居西方，西，阴也，四者，偶之数，亦阴也，故金曰四；土应西南长夏，五者，奇之数，亦阳也，故土曰五。由是论之，则数以阴阳而配者也。若考其深义，则水生于一。天地未分，万物未成之初，莫不先见于水，故《灵枢经》曰：“太一者，水之尊号。先天地之母，后万物之源。”以今验之，草木子实未就，人虫、胎卵、胎胚皆水也，岂不以为一？及其水之聚而形质化，莫不备阴阳 之气在中而后成。故物之小而味苦者，火之兆也；物熟则甘，土之味也。甘极而后淡，淡，本也。然人禀父母阴阳生成之化，故先生二肾，左肾属水，右肾属火。火曰命门，则火之因水而后见。故火曰次二。盖草木子实，大小虽异，其中皆有两以相合者，与人肾同，亦阴阳之兆。是以万物非阴阳合体则不能化生也。]]></content>
      <tags>
        <tag>fate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Plan]]></title>
    <url>%2F2019%2F05%2F31%2Flearning-plan%2F</url>
    <content type="text"><![CDATA[In my blog, i am planning to write something about the following topics: Programming skillsIt will include data structure and algorithum. Besides, there will be some notes about python and c++. Machine learningThese will be about general traditional machine learning algorithums except deep learning, for example xgboost. Deep learningThere will be some notes about deep learning, network structure, tricks of training models, general theory about deep learning. Face recognitionI want to write some notes about face recognition, which will be about the whole pileline of building a face recognition system. Image processBasic knowledge about image process]]></content>
      <categories>
        <category>general notes</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
