<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Cnn Network On Mobile Device]]></title>
    <url>%2F2019%2F06%2F05%2FCnn-Network-on-Mobile-Device%2F</url>
    <content type="text"><![CDATA[1 Depthwise Separable Convolutions1.1 Normal Convolution:Every chanel of the input will be connected to the per channel of the output. If the input size is h_{1} \times w_{1} \times c_{1}, the filters size is h \times w \times c1, the output size is h_{2} \times w_{2} \times c_{2}, the parameter count will be: h \times w \times c_{1} \times c_{2}The example is as follows:input size is 4\times 4 \times 4 and the filters size is 2\times 2\times 4 \times 4, (padding=0, stride=1), output size is 3\times 3 \times 4, so the parameter count is: 2 \times 2 \times4 \times 4=641.2 Group ConvolutionIf the input size is h_{1} \times w_{1} \times c_{1}, the filters size is h\times w \times c_{1}, the output size is h_{2}\times w_{2}\times c_{2}. The input channels and filters will be divided into g groups where each the filters in each filter group are convolved with only \frac{c_{1}}{g} previous layer’s featuremaps, the parameter count will be: h \times w \times \frac{c_{1}}{g} \times \frac{c_{2}}{g} \times gThe example is as follows:input size is 4 \times 4 \times 4 and the input channels are divided into 2 groups, so the size of per group is 4 \times 4 \times \frac{4}{2}, the filters size is 2 \times 2 \times \frac{4}{2}, (padding=0, stride=1), output size is 3 \times 3\times 4, so the parameter count is: 2 \times 2 \times \frac{4}{2} \times \frac{4}{2} \times 2=32Compared with normal convolution with same input/output size, the parameter count of group convolution will reduce to \frac{1}{g}. 1.3 Seperable ConvolutionDepthwise separable convolutions factorize a standard convolution into a depthwise convolution and a 1×1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single filter to each input channel. The pointwise convolution then applies a 1×1 convolution to combine the outputs the depthwise convolution. For instance, a depthwise separable convolution layer takes as input a h_{1} \times w_{1} \times c_{1} feature map and produce a h_{2} \times w_{2} \times c_{2} feature map, the filters size is h \times w \times c_{1}, depthwise convolution’s parameter count is : h \times w \times 1 \times c_{1}and the pointwise convolution’s parameter count is : 1 \times 1 \times c_{1} \times c_{2}So the general parameter count of seperable convolution is h \times w \times 1\times c_{1}+1\times 1 \times c_{1} \times c_{2} , which reduces to \frac{1}{c_{2}}+\frac{1}{h \times w} of standard convolution.The example is as follows: input size is 4 \times 4 \times 4 and depthwise convolution filter size is 2 \times 2 \times 4, output size is 3 \times 3 \times 4 (padding=0, stride=1), so the parameter count is: 2 \times 2 \times 1 \times 4+1 \times 1 \times 4 \times 4=32 2 IGCV Series3 Shufflenet Series4 Mobilenet Series4.1 Mobilenet v1In the mobilenetv1, the core layers that MobileNet is built on is depthwise separable convolution, which is descriped in section 1.3. 4.1.1 Network Structure: The first layer of mobilenetv1 is a full convolution. All layers are followed by a batchnorm and ReLU nonlinearity with the exception of the final fully connected layer which has no nonlinearity and feeds into a softmax layer for classification. Downsampling is handled with strided convolution in the depthwise convolutions as well as in the first layer. A final average pooling reduces the spatial resolution to 1 before the fully connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers. The MobileNet architecture is defined as follows: MobileNetv1 spends 95% of it’s computation time in 1 × 1 convolutions which also has 75% of the parameters. Nearly all of the additional parameters are in the fully connected layer. 4.1.2 Width MultiplierIn order to construct these smaller and less computationally expensive models mobilenetv1 introduce a very simple parameter \alpha called width multiplier. For a given layer, and width multiplier \alpha, the number of input channels M becomes \alpha M and the number of output channels N becomes \alpha N.Width multiplier has the effect of reducing computational cost and the number of parameters quadratically by roughly \alpha^2. 4.1.3 Resolution MultiplierThe input image and the internal representation of every layer is subsequently reduced by the same multiplier. In practice we implicitly set \rho by setting the input resolution. Resolution multiplier has the effect of reducing computational cost by \rho^2. 4.1.4 Experiments Results 4.2 Mobilenet v24.2.1 Depthwise Separable Convolutions DepthwiseMobileNetV2 uses k = 3 (3×3 depthwise separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy. 4.2.2 Linear BottlenecksThe manifold of interest should lie in a low-dimensional subspace of the higher-dimensional activation space: If the manifold of interest remains non-zero vol- ume after ReLU transformation, it corresponds to a linear transformation. ReLU is capable of preserving complete informa- tion about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. These two insights provide us with an empirical hint for optimizing existing neural architectures: assuming the manifold of interest is lowdimensional we can capture this by inserting linear bottleneck layers into the convolutional blocks. 4.2.3 Inverted ResidualsThe motivation for inserting shortcuts is similar to that of classical residual connections: we want to improve the ability of a gradient to propagate across multiplier layers. However, the inverted design is considerably more memory efficient, as well as works slightly better in author’s experiments. The basic implementation structure is as follows, bottleneck residual block transforming from k to k’ channels, with stride s, and expansion factor t,.When the expansion ratio is smaller than 1, this is a classical residual convolutional block. However, the author show that expansion ratio greater than 1 is the most useful. parameters example images 4.2.4 Model Architecture the basic building block is a bottleneck depth-separable convolution with residuals. The architecture of MobileNetV2 contains the initial fully convolution layer with 32 filters, followed by 19 residual bottleneck layers. We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation. We always use kernel size 3×3 as is standard for modern networks, and utilize dropout and batch normalization during training. With the exception of the first layer, we use constantexpansion rate throughout the network. For all our main experiments we use expansion factor of 6 applied to the size of the input tensor. Each line describes a sequence of 1 or more identical layers, repeated n times. All layers in the same sequence have the same number c of output channels. The first layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 × 3 kernels. The expansion factor t is always applied to the input size as described in Table. 4.4.5 Experiment The impact of non-linearities and various types of shortcut (residual) connections. ImageNet Classification 4.3 Mobilenet v34.3.1 Efficient Mobile Building Blocks Mobilenet V2 bottleneck Mobilenet V3 bottleneck Compared with mobilenetv2, there are some changes: add squeeze and excitation add hard sigmoid 4.3.2 Redesigning Feature LayersMobileNetV2’s inverted bottleneck structure and variants use 1x1 convolution as a final layer in order to expand to a higher-dimensional feature space. This layer is critically important in order to have rich features for prediction. However, this comes at a cost of extra latency. To reduce latency and preserve the high dimensional features, we move this layer past the final average pooling. This final set of features is now computed at 1x1 spatial resolution instead of 7x7 spatial resolution. The efficient last stage reduces the latency by 10 milliseconds which is 15% of the running time and reduces the number of operations by 30 millions MAdds with almost no loss of accuracy. 4.3.3 Hard SigmoidA non-linearity called swish was introduced that when used as a drop-in replacement for ReLU, that significantly improves the accuracy of neural networks. The non-linearity is defined as: swish(x) = x · σ(x)We replace sigmoid function with its piece-wise linear hard analog: hswish(x) = x \frac{Relu6(x+3)}{6} First, optimized implementations of ReLU6 are available on virtually all software and hardware frameworks. Second, in quantized mode, it eliminates potential numerical precision loss caused by different implementations of the approximate sig- moid. Finally, even optimized implementations of quan- tized sigmoid tend to be far slower than their ReLU coun- terparts. In our experiments, replacing h-swish with swish in quantized mode increased inference latency by 15%. The cost of applying nonlinearity decreases as we go deeper into the network. We find that most of the benefits swish are realized by using them only in the deeper layers. Thus in our ar- chitectures we only use h-swish at the second half of the model. 4.3.4 MobileNetV3 Architecture mobilenetv3 large mobilenetv3 small 4.3.4 Experiments Floating point performance on Pixel family of phones Quantized performance]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>mobile_net</tag>
        <tag>face_recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fate books]]></title>
    <url>%2F2019%2F06%2F01%2Ffate-books%2F</url>
    <content type="text"><![CDATA[三命汇通论五行生成 天高寥廓，六气回旋以成四时；地厚幽深，五行化生以成万物。可谓无穷而莫测者也。圣人立法以推步者，盖不能逃其数。观其立数之因，亦皆出乎自然。故载于经典，同而不异，推以 达其机，穷以通其变，皆不离于数内。一曰水，二曰火，三曰木，四曰金，五曰土者，咸有所自也。水，北方子之位也，子者，阳之初一，阳数也，故水曰一；火，南方午之位也，午者，阴之初二，阴数也，故火曰二；木居东方，东，阳也，三者，奇之数，亦阳也，故木曰三；金居西方，西，阴也，四者，偶之数，亦阴也，故金曰四；土应西南长夏，五者，奇之数，亦阳也，故土曰五。由是论之，则数以阴阳而配者也。若考其深义，则水生于一。天地未分，万物未成之初，莫不先见于水，故《灵枢经》曰：“太一者，水之尊号。先天地之母，后万物之源。”以今验之，草木子实未就，人虫、胎卵、胎胚皆水也，岂不以为一？及其水之聚而形质化，莫不备阴阳 之气在中而后成。故物之小而味苦者，火之兆也；物熟则甘，土之味也。甘极而后淡，淡，本也。然人禀父母阴阳生成之化，故先生二肾，左肾属水，右肾属火。火曰命门，则火之因水而后见。故火曰次二。盖草木子实，大小虽异，其中皆有两以相合者，与人肾同，亦阴阳之兆。是以万物非阴阳合体则不能化生也。]]></content>
      <tags>
        <tag>fate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Plan]]></title>
    <url>%2F2019%2F05%2F31%2Flearning-plan%2F</url>
    <content type="text"><![CDATA[In my blog, i am planning to write something about the following topics: Programming skillsIt will include data structure and algorithum. Besides, there will be some notes about python and c++. Machine learningThese will be about general traditional machine learning algorithums except deep learning, for example xgboost. Deep learningThere will be some notes about deep learning, network structure, tricks of training models, general theory about deep learning. Face recognitionI want to write some notes about face recognition, which will be about the whole pileline of building a face recognition system. Image processBasic knowledge about image process]]></content>
      <categories>
        <category>general notes</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>plan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
